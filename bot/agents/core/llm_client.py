from typing import Optional
import hashlib
from ..utils import mask_pii  # ‚Üê –∏—Å–ø–æ–ª—å–∑—É–µ–º –°–£–©–ï–°–¢–í–£–Æ–©–£–Æ —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ bot/utils.py

class LLMClient:
    """
    –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—ã–∑–æ–≤–∞ LLM —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ü–î–Ω.
    """
    def __init__(self, groq_client):
        self.groq_client = groq_client
        self._cache = {}

    async def call_llm(
        self,
        system_prompt: str,
        user_query: str,
        model: str = "llama-3.1-8b-instant",
        max_tokens: int = 2000
    ) -> Optional[str]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Groq —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π –ü–î–Ω.
        """
        # üîí –ú–ê–°–ö–ò–†–û–í–ö–ê –ü–î–Ω ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê
        clean_query = mask_pii(user_query)

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        cache_key = hashlib.md5(f"{system_prompt}:{clean_query}".encode()).hexdigest()
        if cache_key in self._cache:
            return self._cache[cache_key]

        try:
            response = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": clean_query}
                ],
                model=model,
                max_tokens=max_tokens,
                temperature=0.7
            )
            result = response.choices[0].message.content
            self._cache[cache_key] = result
            return result
        except Exception as e:
            print(f"LLMClient error: {e}")
            return None
