"""
–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—ã–∑–æ–≤–∞ LLM –¥–ª—è UAF-–∞–≥–µ–Ω—Ç–æ–≤.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, –º–∞—Å–∫–∏—Ä–æ–≤–∫—É –ü–î–Ω –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å.
"""
from typing import Optional
import hashlib
from bot.utils import mask_pii  # ‚Üê –ø—Ä–∞–≤–∏–ª—å–Ω–æ: –∏–º–ø–æ—Ä—Ç –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –ø–∞–∫–µ—Ç–∞ bot
from bot.models import ai_cache  # ‚Üê –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π –∫—ç—à –∏–∑ models.py


class LLMClient:
    """
    –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—ã–∑–æ–≤–∞ LLM —Å –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ü–î–Ω –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.
    """
    def __init__(self, groq_client):
        self.groq_client = groq_client

    async def call_llm(
        self,
        system_prompt: str,
        user_query: str,
        model: str = "llama-3.1-8b-instant",
        max_tokens: int = 2000
    ) -> Optional[str]:
        """
        –í—ã–∑—ã–≤–∞–µ—Ç Groq —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –º–∞—Å–∫–∏—Ä–æ–≤–∫–æ–π –ü–î–Ω –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–≤–µ—Ç–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π `ai_cache` –∏–∑ `models.py` –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –±–æ—Ç–æ–º.
        """
        # üîí –ú–ê–°–ö–ò–†–û–í–ö–ê –ü–î–Ω ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–ê
        clean_query = mask_pii(user_query)

        # üî• –ò–°–ü–û–õ–¨–ó–£–ï–ú –ì–õ–û–ë–ê–õ–¨–ù–´–ô –ö–≠–® (–∫–∞–∫ –≤ ai_handlers.py)
        cache_key = ai_cache.get_cache_key("orchestrator", clean_query)
        cached_response = ai_cache.get_cached_response("orchestrator", clean_query)
        if cached_response:
            return cached_response

        try:
            response = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": clean_query}
                ],
                model=model,
                max_tokens=max_tokens,
                temperature=0.7
            )
            result = response.choices[0].message.content

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ–±—â–∏–π –∫—ç—à
            ai_cache.cache_response("orchestrator", clean_query, result)
            return result

        except Exception as e:
            print(f"LLMClient error: {e}")
            return None
